{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3. Свёрточные и рекуррентные нейронные сети\n",
    "\n",
    "## Овсепян А.Н. ИУ8-83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В зависимости от варианта, для решения задачи используйте следующие наборы данных:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "         <tr>\n",
    "            <th colspan=1>Задача генерации</th>\n",
    "            <th colspan=5>Задача классификации</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th> </th>\n",
    "            <th>A</th>\n",
    "            <th>B</th>\n",
    "            <th>C</th>\n",
    "            <th>D</th>\n",
    "            <th>E</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>K</td>\n",
    "            <td>1</td>\n",
    "            <td>2</td>\n",
    "            <td>3</td>\n",
    "            <td>4</td>\n",
    "            <td>5</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>L</td>\n",
    "            <td>6</td>\n",
    "            <td>7</td>\n",
    "            <td>8</td>\n",
    "            <td>9</td>\n",
    "            <td>10</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>M</td>\n",
    "            <td>11</td>\n",
    "            <td>12</td>\n",
    "            <td>13</td>\n",
    "            <td>14</td>\n",
    "            <td>15</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>N</td>\n",
    "            <td>16</td>\n",
    "            <td>17</td>\n",
    "            <td>18</td>\n",
    "            <td>19</td>\n",
    "            <td>20</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>O</td>\n",
    "            <td>21</td>\n",
    "            <td>22</td>\n",
    "            <td>23</td>\n",
    "            <td>24</td>\n",
    "            <td>25</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "### Наборы данных\n",
    "\n",
    "**Задача классификации картинок**\n",
    "\n",
    "A. [Набор данных \"Fashion MNIST\"](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) `torchvision.datasets.FashionMNIST train=True и False`\n",
    "\n",
    "B. [Набор данных \"Текстуры\"](https://pytorch.org/vision/main/generated/torchvision.datasets.DTD.html#torchvision.datasets.DTD) `torchvision.datasets.DTD split='train' и 'test'`\n",
    "\n",
    "C. [Набор данных \"Буквы\"](https://pytorch.org/vision/main/generated/torchvision.datasets.EMNIST.html#torchvision.datasets.EMNIST) `torchvision.datasets.EMNIST split=letters train=True и False`\n",
    "\n",
    "D. [Набор данных \"Кошки и собаки\"](https://pytorch.org/vision/main/generated/torchvision.datasets.OxfordIIITPet.html) `torchvision.datasets.OxfordIIITPet split = 'trainval' и 'test'` (объекты разбиты по породам (всего 37 пород), нужно построить бинарный классификатор собак и кошек)\n",
    "\n",
    "E. [Набор данных \"Дорожные знаки\"](https://pytorch.org/vision/main/generated/torchvision.datasets.GTSRB.html) `torchvision.datasets.GTSRB split = 'train' и 'test'`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Задача генерации текстов**\n",
    "\n",
    "K. [Набор данных \"Евгений Онегин\"]() `onegin.txt`\n",
    "\n",
    "L. [Набор данных \"Война и Мир\"]() `war_and_piece.txt`\n",
    "\n",
    "M. [Набор данных \"Стихи\"]() `stihi.csv`\n",
    "\n",
    "N. [Набор данных \"Сказки\"]() `fairytales.txt`\n",
    "\n",
    "O. [Набор данных \"Новости\"]() `news.txt` или https://github.com/yutkin/Lenta.Ru-News-Dataset/releases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Инициализация библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catsndogs = torchvision.datasets.OxfordIIITPet(root='data_train', download=True).classes\n",
    "\n",
    "cats = ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British Shorthair', 'Egyptian Mau', 'Maine Coon', 'Persian', 'Ragdoll', 'Russian Blue', 'Siamese', 'Sphynx']\n",
    "\n",
    "dogs = ['American Bulldog', 'American Pit Bull Terrier', 'Basset Hound', 'Beagle', 'Boxer', 'Chihuahua', 'English Cocker Spaniel', 'English Setter', 'German Shorthaired', 'Great Pyrenees',\n",
    "        'Havanese', 'Japanese Chin', 'Keeshond', 'Leonberger', 'Miniature Pinscher', 'Newfoundland', 'Pomeranian', 'Pug', 'Saint Bernard', 'Samoyed', 'Scottish Terrier', 'Shiba Inu',\n",
    "        'Staffordshire Bull Terrier', 'Wheaten Terrier', 'Yorkshire Terrier']\n",
    "\n",
    "cats_num=[]\n",
    "for x in catsndogs:\n",
    "  if x in cats:\n",
    "    cats_num.append(catsndogs.index(x))\n",
    "  else:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Свёрточная нейронная сеть для классификации изображений (9 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Загрузка данных, разделение на train и test\n",
    "\n",
    "Загрузите данные при помощи torchvision.datasets. Можете предусмотреть опредлённые транфсофрмации картинки, например, изменение размера до 128 px, кроп по центру в 196 px и преобразование к классу torch.Tensor:\n",
    "\n",
    "```\n",
    "import torchvision\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    # resize\n",
    "    torchvision.transforms.Resize(128),\n",
    "    torchvision.transforms.CenterCrop(196),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((75, 75)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "target_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Lambda(lambda x: int(x in cats_num))\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.OxfordIIITPet(\n",
    "    root='data_train',\n",
    "    split='trainval',\n",
    "    transform=data_transforms,\n",
    "    target_transform=target_transform,\n",
    "    download=True),\n",
    "    shuffle=False, \n",
    "    batch_size=64\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    torchvision.datasets.OxfordIIITPet(\n",
    "        'data_train', \n",
    "        download=True, \n",
    "        split='test', \n",
    "        transform=data_transforms,\n",
    "        target_transform=target_transform),\n",
    "    shuffle=False,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Предобработка данных (по необходимости)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_loader:\n",
    "    print(type(x), type(x[0]), x[0].shape, type(x[1]), x[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Инициализация объекта свёрточной нейронной сети\n",
    "\n",
    "Создайте нейронную сеть с 2-4 свёрточными слоями. В остальном конфигурация сети может быть произвольной. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,100, 5)\n",
    "        self.conv1_bn = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(100, 150, 3)\n",
    "        self.conv2_bn = nn.BatchNorm2d(150)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(150, 250, 1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(250)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16000, 350)\n",
    "        self.fc1_bn = nn.BatchNorm1d(350)\n",
    "        \n",
    "        self.fc2 = nn.Linear(350, 2)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pool(F.elu(self.conv1(x)))\n",
    "        x = self.dropout(self.conv1_bn(x))\n",
    "        \n",
    "\n",
    "        x = self.pool(F.elu(self.conv2(x)))\n",
    "        x = self.dropout(self.conv2_bn(x))\n",
    "\n",
    "          \n",
    "        x = self.pool(F.elu(self.conv3(x)))\n",
    "        x = self.dropout(self.conv3_bn(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(self.fc1_bn(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device)\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": Accuracy(),\n",
    "    \"loss\": Loss(criterion)\n",
    "}\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)\n",
    "val_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 50\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}], Iter[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    train_evaluator.run(train_loader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    print(f\"Training Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    val_evaluator.run(test_loader)\n",
    "    metrics = val_evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(engine):\n",
    "    return engine.state.metrics[\"accuracy\"]\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"checkpoint\",\n",
    "    n_saved=2,\n",
    "    filename_prefix=\"best\",\n",
    "    score_function=score_function,\n",
    "    score_name=\"accuracy\",\n",
    "    global_step_transform=global_step_from_engine(trainer),\n",
    ")\n",
    "  \n",
    "val_evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Обучение сввёрточной нейронной сети для решения задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tb_logger = TensorboardLogger(log_dir=f'tb-logger/run_net_{int(time.time())}')\n",
    "\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED(every=log_interval),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batch_loss\": loss},\n",
    ")\n",
    "\n",
    "\n",
    "for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", val_evaluator)]:\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=tag,\n",
    "        metric_names=\"all\",\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(train_loader, max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Агументация данных и регуляризация нейросети\n",
    "\n",
    "Попобуйте добавить аугументацию (случайные повороты, увеличение, уменьшение) в данные, а также регуляризацию (Drop out и Batch Norm слои) в модель. Как меняется метрика accuracy на обучающей и тестовой выборке в процессе обучения? Постройте график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((75, 75)),\n",
    "    torchvision.transforms.RandomRotation((-45, 45)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "target_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Lambda(lambda x: int(x in cats_num))\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.OxfordIIITPet(\n",
    "    root='data_train',\n",
    "    split='trainval',\n",
    "    transform=data_transforms,\n",
    "    target_transform=target_transform,\n",
    "    download=True),\n",
    "    shuffle=False, \n",
    "    batch_size=64\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    torchvision.datasets.OxfordIIITPet(\n",
    "        'data_train', \n",
    "        download=True, \n",
    "        split='test', \n",
    "        transform=data_transforms,\n",
    "        target_transform=target_transform),\n",
    "    shuffle=False,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Обучение нейросети на данных с аугументацией "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device)\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": Accuracy(),\n",
    "    \"loss\": Loss(criterion)\n",
    "}\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)\n",
    "val_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 50\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}], Iter[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    train_evaluator.run(train_loader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    print(f\"Training Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    val_evaluator.run(test_loader)\n",
    "    metrics = val_evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(engine):\n",
    "    return engine.state.metrics[\"accuracy\"]\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"checkpoint_aug\",\n",
    "    n_saved=2,\n",
    "    filename_prefix=\"best_aug\",\n",
    "    score_function=score_function,\n",
    "    score_name=\"accuracy\",\n",
    "    global_step_transform = global_step_from_engine(trainer),\n",
    ")\n",
    "  \n",
    "val_evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tb_logger = TensorboardLogger(log_dir=f'tb-logger/run_aug_{int(time.time())}')\n",
    "\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED(every=log_interval),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batch_loss\": loss},\n",
    ")\n",
    "\n",
    "for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", val_evaluator)]:\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=tag,\n",
    "        metric_names=\"all\",\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )\n",
    "\n",
    "trainer.run(train_loader, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Рекуррентная нейронная сеть для генерации текстов (9 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/content/content/MyDrive/dataset/L_war_and_piece.txt') as text_file:\n",
    "    text_sample = text_file.readlines()\n",
    "text_sample = ' '.join(text_sample)\n",
    "\n",
    "def text_to_seq(text_sample):\n",
    "    char_counts = Counter(text_sample)\n",
    "    char_counts = sorted(char_counts.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    sorted_chars = [char for char, _ in char_counts]\n",
    "    print(sorted_chars)\n",
    "    char_to_idx = {char: index for index, char in enumerate(sorted_chars)}\n",
    "    idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "    sequence = np.array([char_to_idx[char] for char in text_sample])\n",
    "    \n",
    "    return sequence, char_to_idx, idx_to_char\n",
    "\n",
    "sequence, char_to_idx, idx_to_char = text_to_seq(text_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Преодбработка/нормализация данных по необходимости\n",
    "\n",
    "Например, можно привести все слова к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_seq = 256\n",
    "batch = 16\n",
    "\n",
    "def get_batch(sequence):\n",
    "    trains = []\n",
    "    targets = []\n",
    "    for _ in range(batch):\n",
    "        batch_start = np.random.randint(0, len(sequence) - len_seq)\n",
    "        chunk = sequence[batch_start: batch_start + len_seq]\n",
    "        train = torch.LongTensor(chunk[:-1]).view(-1, 1)\n",
    "        target = torch.LongTensor(chunk[1:]).view(-1, 1)\n",
    "        trains.append(train)\n",
    "        targets.append(target)\n",
    "    return torch.stack(trains, dim=0), torch.stack(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
    "    hidden = model.init_hidden()\n",
    "    idx_input = [char_to_idx[char] for char in start_text]\n",
    "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
    "    predicted_text = start_text\n",
    "    \n",
    "    _, hidden = model(train, hidden)\n",
    "        \n",
    "    inp = train[-1].view(-1, 1, 1)\n",
    "    \n",
    "    for i in range(prediction_len):\n",
    "        output, hidden = model(inp.to(device), hidden)\n",
    "        output_logits = output.cpu().data.view(-1)\n",
    "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()        \n",
    "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
    "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
    "        predicted_char = idx_to_char[top_index]\n",
    "        predicted_text += predicted_char\n",
    "    \n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Инициализация объекта рекуррентной нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = len(idx_to_char)\n",
    "        self.hidden_size = 128\n",
    "        self.embedding_size = 128\n",
    "        self.n_layers = 2\n",
    "\n",
    "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.n_layers)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.input_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.encoder(x).squeeze(2)\n",
    "        out, ht1 = self.rnn(x, hidden)\n",
    "        out = self.dropout(out)\n",
    "        x = self.fc(out)\n",
    "        return x, ht1\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Обучение обычной рекуррентной нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = TextRNN(input_size=len(idx_to_char), hidden_size=128, embedding_size=128, n_layers=2)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience=5, \n",
    "    verbose=True, \n",
    "    factor=0.5\n",
    ")\n",
    "\n",
    "n_epochs = 10000\n",
    "loss_avg = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train, target = get_batch(sequence)\n",
    "    train = train.permute(1, 0, 2).to(device)\n",
    "    target = target.permute(1, 0, 2).to(device)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "\n",
    "    output, hidden = model(train, hidden)\n",
    "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_avg.append(loss.item())\n",
    "\n",
    "    if epoch%500 == 0:\n",
    "        print(\"\\nEpoch number: \", epoch)\n",
    "        mean_loss = np.mean(loss_avg)\n",
    "        print(f'Loss: {mean_loss}')\n",
    "        scheduler.step(mean_loss)\n",
    "        model.eval()\n",
    "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
    "        print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Генерация текстов при помощи обычной рекуррентной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.eval()\n",
    "print(evaluate(\n",
    "    model, \n",
    "    char_to_idx, \n",
    "    idx_to_char, \n",
    "    temp=0.3, \n",
    "    prediction_len=200, \n",
    "    start_text='Большой дуб '\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Обучение рекуррентной нейронной сети с LSTM-ячейками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
    "    hidden = model.init_hidden()\n",
    "    idx_input = [char_to_idx[char] for char in start_text]\n",
    "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
    "    predicted_text = start_text\n",
    "    \n",
    "    _, hidden = model(train, hidden)\n",
    "        \n",
    "    inp = train[-1].view(-1, 1, 1)\n",
    "    \n",
    "    for i in range(prediction_len):\n",
    "        output, hidden = model(inp.to(device), hidden)\n",
    "        output_logits = output.cpu().data.view(-1)\n",
    "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()        \n",
    "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
    "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
    "        predicted_char = idx_to_char[top_index]\n",
    "        predicted_text += predicted_char\n",
    "    \n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTSM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LTSM, self).__init__()\n",
    "        \n",
    "        self.input_size = len(idx_to_char)\n",
    "        self.hidden_size = 128\n",
    "        self.embedding_size = 128\n",
    "        self.n_layers = 2\n",
    "\n",
    "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.n_layers)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.input_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.encoder(x).squeeze(2)\n",
    "        out, (ht1, ct1) = self.lstm(x, hidden)\n",
    "        out = self.dropout(out)\n",
    "        x = self.fc(out)\n",
    "        return x, (ht1, ct1)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),\n",
    "               torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LTSM()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience=5, \n",
    "    verbose=True, \n",
    "    factor=0.5\n",
    ")\n",
    "\n",
    "n_epochs = 10000\n",
    "loss_avg = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train, target = get_batch(sequence)\n",
    "    train = train.permute(1, 0, 2).to(device)\n",
    "    target = target.permute(1, 0, 2).to(device)\n",
    "    hidden = model.init_hidden(batch)\n",
    "\n",
    "    output, hidden = model(train, hidden)\n",
    "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_avg.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.eval()\n",
    "print(evaluate(\n",
    "    model, \n",
    "    char_to_idx, \n",
    "    idx_to_char, \n",
    "    temp=0.3, \n",
    "    prediction_len=200, \n",
    "    start_text='Большой дуб '\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Генерация текстов с разной температурой\n",
    "\n",
    "Попробуйте сгенерировать тексты с температурой из списка: `[0.1, 0.2, 0.4, 0.6, 0.8, 1.0]` (см. нотубук с семинара по RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "for temp in temps:\n",
    "  model.eval()\n",
    "  print(evaluate(\n",
    "      model, \n",
    "      char_to_idx, \n",
    "      idx_to_char, \n",
    "      temp=temp, \n",
    "      prediction_len=200, \n",
    "      start_text='Большой дуб '\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Выводы"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
